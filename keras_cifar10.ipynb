{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/studentuser/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/studentuser/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.keras.api.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(tf.keras.callbacks.TensorBoard):\n",
    "    #  code posted by VladislavZavadskyy at https://github.com/keras-team/keras/issues/6692\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter += 1\n",
    "        if self.counter % self.log_every == 0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "\n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below is where we define the folder we want to save the logs to, the number of epochs to train over, the number of items to train over before updating the weights, and whether or not you wish to save the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = \"/tmp/IntelAIworkshop/\"\n",
    "nbEpochs = int(1)\n",
    "batchSize = int(32)\n",
    "modelSave = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are installing/loading in the data. The x_train and x_test variables contain the image data for the training and validation files respectively. The y_train and y_test variables contain the associated labels for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contains pixel values between 0 and 255. Here we scale the data to values between 0 and 1. We do this primarily to slightly decrease training time. The same scaling is applied to both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max scale all the image data\n",
    "training_scaled = x_train / x_train.max()\n",
    "test_scaled = x_test / x_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are numerical values between 0 and 9. The images are sparsely coded, meaning each image can only be apart of a *single* category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of labels\n",
    "labels_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "labels_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: 4 convolutional layers, 2 max pools\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin adding layers onto the model. Our first layer after the input will be a 2-dimensional convolutional layer. \n",
    "\n",
    "We first define the number of feature maps that this convolutional layer will output, which will be 32. \n",
    "\n",
    "We then define the size of the receptive window that will pass over the input matrix starting from the top left and finishing at the bottom right. \n",
    "\n",
    "The **padding** arguement means that the output feature maps will be zero-padded to the same size of the input.\n",
    "\n",
    "In Keras, we have to identify the size of the input in the first layer of the network. This is defined by the **input_shape** arguement. \n",
    "\n",
    "The **strides** arguement defines how far the receptive window moves and determines the overlap in your features. Our receptive window will be moving 1 pixel to the right and after it reaches the end of the row, it will move 1 pixel down. This essentially provides the maximal overlap in our features.\n",
    "\n",
    "The activation function used in the convolutional layer can be defined inside or outside the layer call. Here we are using the *rectified linear* (relu) function. This is the best general purpose activation function, but is prone to creating \"dead\" nodes. If you get a lot of dead nodes, injecting *truncated normal* noise can revive them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1'):\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=training_scaled.shape[1:],\n",
    "                     strides=(1, 1),\n",
    "                     # kernel_initializer='TruncatedNormal',\n",
    "                     name='conv1'))\n",
    "    model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv2'):\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     strides=(1, 1),\n",
    "                     # kernel_initializer='TruncatedNormal',\n",
    "                     name='conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "model.add(Dropout(0.25, name='dropout1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv3'):\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                     strides=(1, 1),\n",
    "                     # kernel_initializer='TruncatedNormal',\n",
    "                     name='conv3'))\n",
    "    model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv4'):\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                     strides=(1, 1),\n",
    "                     # kernel_initializer='TruncatedNormal',\n",
    "                     name='conv4'))\n",
    "    model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='pool2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten(name='flat1'))  # converts feature maps to feature vectors\n",
    "\n",
    "with tf.variable_scope('fc1'):\n",
    "    model.add(Dense(1024,\n",
    "                    # kernel_initializer='TruncatedNormal',\n",
    "                    name='fc1'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5, name='dropout2'))  # reset half of the weights to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('output'):\n",
    "    model.add(Dense(10,\n",
    "                    name='output'))\n",
    "    model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TB(log_dir=LOGDIR,\n",
    "                 histogram_freq=1,\n",
    "                 batch_size=20,\n",
    "                 write_graph=True,\n",
    "                 write_grads=False,\n",
    "                 write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing the model...\")\n",
    "# fits the model on batches, waits to send the error back after the number of batches\n",
    "model.fit(training_scaled,\n",
    "          labels_train,\n",
    "          batch_size=batchSize,\n",
    "          epochs=nbEpochs,\n",
    "          validation_data=(test_scaled, labels_test),\n",
    "          verbose=1,\n",
    "          callbacks=[tensorboard])\n",
    "\n",
    "if modelSave:\n",
    "    model.save('./cifarClassification.h5')\n",
    "\n",
    "print('To run tensorboard, open up either Firefox or Chrome and type localhost:6006 in the address bar.')\n",
    "print('Then run `tensorboard --logdir=%s` in your terminal.' % LOGDIR)\n",
    "print('If youre on a Mac, provide the following flag: '\n",
    "      '--host=localhost to the previous terminal string.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
